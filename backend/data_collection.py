"""
Script de collecte de donn√©es sur l'entrepreneuriat au Burkina Faso
Projet : Assistant IA Contextuel - Hackathon 2025
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
import PyPDF2
from urllib.parse import urljoin, urlparse

class EntrepreneurshipDataCollector:
    def __init__(self):
        self.corpus = []
        self.sources = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Cr√©er les dossiers n√©cessaires
        os.makedirs('data', exist_ok=True)
        os.makedirs('data/pdfs', exist_ok=True)
    
    def scrape_article(self, url, category="entrepreneuriat"):
        """Scrape un article web"""
        try:
            print(f"üìÑ Scraping: {url}")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraire le titre
            title = ""
            for tag in ['h1', 'h2', '.article-title', '.entry-title']:
                if soup.find(tag):
                    title = soup.find(tag).get_text().strip()
                    break
            
            # Extraire le contenu
            content = ""
            
            # M√©thode 1: Chercher div article/content
            article_divs = soup.find_all(['article', 'div'], class_=['article', 'content', 'entry-content', 'post-content'])
            if article_divs:
                paragraphs = article_divs[0].find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs])
            
            # M√©thode 2: Tous les paragraphes si rien trouv√©
            if not content:
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs[:20]])
            
            # Nettoyer le contenu
            content = ' '.join(content.split())
            
            if len(content) < 100:
                print(f"‚ö†Ô∏è  Contenu trop court, ignor√©")
                return None
            
            document = {
                "id": len(self.corpus) + 1,
                "title": title or "Sans titre",
                "content": content,
                "source": urlparse(url).netloc,
                "url": url,
                "date": datetime.now().strftime("%Y-%m-%d"),
                "category": category,
                "type": "web"
            }
            
            self.corpus.append(document)
            self.sources.append(url)
            print(f"‚úÖ Collect√©: {title[:50]}...")
            return document
            
        except Exception as e:
            print(f"‚ùå Erreur avec {url}: {str(e)}")
            return None
    
    def scrape_lefaso_entrepreneuriat(self, max_pages=10):
        """Scrape articles entrepreneuriat de Lefaso.net"""
        print("\nüîç Scraping Lefaso.net...")
        base_url = "https://lefaso.net"
        
        # URLs d'exemple - √† adapter selon la structure r√©elle du site
        urls = [
            f"{base_url}/spip.php?page=recherche&recherche=entrepreneuriat",
            f"{base_url}/spip.php?page=recherche&recherche=creation+entreprise",
            f"{base_url}/spip.php?page=recherche&recherche=startup",
            f"{base_url}/spip.php?page=recherche&recherche=entrepreneur",
        ]
        
        for url in urls[:max_pages]:
            self.scrape_article(url, "entrepreneuriat")
            time.sleep(2)
    
    def download_pdf(self, url, filename):
        """T√©l√©charge un PDF"""
        try:
            print(f"üì• T√©l√©chargement PDF: {filename}")
            response = requests.get(url, headers=self.headers, timeout=30)
            
            filepath = f"data/pdfs/{filename}"
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"‚úÖ PDF t√©l√©charg√©: {filename}")
            return filepath
        except Exception as e:
            print(f"‚ùå Erreur t√©l√©chargement {filename}: {str(e)}")
            return None
    
    def extract_text_from_pdf(self, pdf_path):
        """Extrait le texte d'un PDF"""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()
                return text
        except Exception as e:
            print(f"‚ùå Erreur extraction PDF: {str(e)}")
            return ""
    
    def process_pdfs(self):
        """Traite tous les PDFs t√©l√©charg√©s"""
        print("\nüìö Traitement des PDFs...")
        
        pdf_files = [f for f in os.listdir('data/pdfs') if f.endswith('.pdf')]
        
        for pdf_file in pdf_files:
            pdf_path = f"data/pdfs/{pdf_file}"
            text = self.extract_text_from_pdf(pdf_path)
            
            if len(text) > 200:
                document = {
                    "id": len(self.corpus) + 1,
                    "title": pdf_file.replace('.pdf', '').replace('_', ' '),
                    "content": text,
                    "source": "PDF Document",
                    "url": pdf_path,
                    "date": datetime.now().strftime("%Y-%m-%d"),
                    "category": "entrepreneuriat",
                    "type": "pdf"
                }
                self.corpus.append(document)
                print(f"‚úÖ PDF trait√©: {pdf_file}")
    
    def scrape_multiple_urls(self, urls_dict):
        """Scrape une liste d'URLs avec leurs cat√©gories"""
        print("\nüåê Scraping URLs multiples...")
        
        for category, urls in urls_dict.items():
            print(f"\nüìÇ Cat√©gorie: {category}")
            for url in urls:
                self.scrape_article(url, category)
                time.sleep(2)  # Respecter les serveurs
    
    def generate_synthetic_data(self, count=50):
        """G√©n√®re des donn√©es synth√©tiques pour compl√©ter le corpus"""
        print(f"\nü§ñ G√©n√©ration de {count} documents synth√©tiques...")
        
        topics = [
            "Cr√©ation d'entreprise au Burkina Faso",
            "Financement des startups burkinab√®",
            "Fiscalit√© pour entrepreneurs au Burkina",
            "Success story entrepreneur burkinab√®",
            "Microcr√©dits et entrepreneuriat",
            "Incubateurs et acc√©l√©rateurs √† Ouagadougou",
            "Secteurs porteurs au Burkina Faso",
            "APEJ et accompagnement des jeunes entrepreneurs",
            "Formalit√©s CEFORE cr√©ation entreprise",
            "Entrepreneuriat f√©minin au Burkina"
        ]
        
        for i in range(count):
            topic = topics[i % len(topics)]
            document = {
                "id": len(self.corpus) + 1,
                "title": f"{topic} - Article {i+1}",
                "content": f"Contenu d√©taill√© sur {topic}. Ce document couvre les aspects essentiels de l'entrepreneuriat au Burkina Faso, incluant les d√©marches administratives, les opportunit√©s de financement, et les conseils pratiques pour r√©ussir dans le contexte burkinab√®. Les entrepreneurs doivent tenir compte des sp√©cificit√©s locales et des ressources disponibles.",
                "source": "synthetic_data",
                "url": f"synthetic_{i+1}",
                "date": datetime.now().strftime("%Y-%m-%d"),
                "category": "entrepreneuriat",
                "type": "synthetic"
            }
            self.corpus.append(document)
    
    def save_corpus(self):
        """Sauvegarde le corpus en JSON"""
        print("\nüíæ Sauvegarde du corpus...")
        
        with open('data/corpus.json', 'w', encoding='utf-8') as f:
            json.dump(self.corpus, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Corpus sauvegard√©: {len(self.corpus)} documents")
    
    def save_sources(self):
        """Sauvegarde la liste des sources"""
        print("\nüìù Sauvegarde des sources...")
        
        with open('data/sources.txt', 'w', encoding='utf-8') as f:
            f.write("SOURCES UTILIS√âES POUR LE CORPUS - ENTREPRENEURIAT BURKINA FASO\n")
            f.write("=" * 70 + "\n\n")
            
            f.write("DOMAINE: Entrepreneuriat au Burkina Faso\n")
            f.write(f"DATE DE COLLECTE: {datetime.now().strftime('%Y-%m-%d')}\n")
            f.write(f"NOMBRE TOTAL DE DOCUMENTS: {len(self.corpus)}\n\n")
            
            # Grouper par type
            web_docs = [d for d in self.corpus if d['type'] == 'web']
            pdf_docs = [d for d in self.corpus if d['type'] == 'pdf']
            synthetic_docs = [d for d in self.corpus if d['type'] == 'synthetic']
            
            f.write(f"R√âPARTITION:\n")
            f.write(f"- Articles web: {len(web_docs)}\n")
            f.write(f"- Documents PDF: {len(pdf_docs)}\n")
            f.write(f"- Donn√©es synth√©tiques: {len(synthetic_docs)}\n\n")
            
            f.write("SITES WEB SCRAP√âS:\n")
            f.write("-" * 70 + "\n")
            unique_sources = list(set([d['source'] for d in web_docs]))
            for source in unique_sources:
                f.write(f"- {source}\n")
            
            f.write("\nDOCUMENTS PDF:\n")
            f.write("-" * 70 + "\n")
            for doc in pdf_docs:
                f.write(f"- {doc['title']}\n")
            
            f.write("\nCAT√âGORIES COUVERTES:\n")
            f.write("-" * 70 + "\n")
            categories = list(set([d['category'] for d in self.corpus]))
            for cat in categories:
                count = len([d for d in self.corpus if d['category'] == cat])
                f.write(f"- {cat}: {count} documents\n")
        
        print(f"‚úÖ Sources sauvegard√©es dans data/sources.txt")
    
    def print_statistics(self):
        """Affiche les statistiques de collecte"""
        print("\n" + "=" * 70)
        print("üìä STATISTIQUES DE COLLECTE")
        print("=" * 70)
        print(f"Total documents collect√©s: {len(self.corpus)}")
        print(f"Documents web: {len([d for d in self.corpus if d['type'] == 'web'])}")
        print(f"Documents PDF: {len([d for d in self.corpus if d['type'] == 'pdf'])}")
        print(f"Documents synth√©tiques: {len([d for d in self.corpus if d['type'] == 'synthetic'])}")
        print("=" * 70)


def main():
    """Fonction principale de collecte"""
    print("üöÄ COLLECTE DE DONN√âES - ENTREPRENEURIAT BURKINA FASO")
    print("=" * 70)
    
    collector = EntrepreneurshipDataCollector()
    
    # √âTAPE 1: URLs sp√©cifiques √† scraper
    
    urls_to_scrape = {
    "creation_entreprise": [
        "https://servicepublic.gov.bf/fiches/creation‚Äëdentreprise‚Äëdemande‚Äëde‚Äëcreation‚Äëdentreprises‚Äëpour‚Äëles‚Äëpersonnes‚Äëmorales",
        "https://servicepublic.gov.bf/fiches/creation‚Äëdentreprise‚Äëdemande‚Äëde‚Äëcreation‚Äëentreprises‚Äëpour‚Äëles‚Äëpersonnes‚Äëphysiques",
        "https://servicepublic.gov.bf/entreprises/entreprenariat/creation‚Äëdentreprise",
        "https://servicepublic.gov.bf/eservice/demande‚Äëde‚Äëcreation‚Äëdentreprises‚Äëpour‚Äëles‚Äëpersonnes‚Äëmorales‚Äëou‚Äëphysiques",
        "https://servicepublic.gov.bf/fiches/creation‚Äëdentreprise‚Äëdemande‚Äëdautorisation‚Äëdimplantation‚Äëdunites‚Äëindustrielles‚Äëautre‚Äëque‚Äëles‚Äëunites‚Äëdensachage‚Äëdeau‚Äëet‚Äëles‚Äëunites‚Äëde‚Äëproduction‚Äëdhuiles‚Äëalimentaires",
        "https://legafrik.com/cr%C3%A9ez‚Äëvotre‚Äëentreprise‚Äëindividuelle‚Äëau‚Äëburkina‚Äëfaso‚Äëen‚Äëtoute‚Äërapidit%C3%A9",
        "https://biznesskibaya.com/comment‚Äëcreer‚Äëune‚Äëentreprise‚Äëau‚Äëburkina‚Äëfaso/"
    ],
    "financement": [
        "https://servicepublic.gov.bf/fiches/formation‚Äëprofessionnelle‚Äëformation‚Äëen‚Äëentreprenariat",
        "https://servicepublic.gov.bf/fiches/emploi‚Äëdemande‚Äëde‚Äëfinancement‚Äëde‚Äëmicro‚Äëprojets‚Äëdu‚Äësecteur‚Äëinformel",
        "https://investirauburkina.net/secteurs‚Äëet‚Äëmarches/finances/financer‚Äëson‚Äëprojet‚Äëdentreprise‚Äëau‚Äëburkina‚Äëfaso‚Äëou‚Äëtrouver‚Äëlargent.html",
        "https://afppme.bf/",
        "https://acep-bf.com/",
        "https://sinergiburkina.com/",
        "https://faij.gov.bf/presentation",
        "https://www.international.gc.ca/world-monde/funding-financement/cfli-fcil/burkina-faso.aspx?lang=fra"
    ],
    "fiscalite": [
        "https://dgi.bf/verification/CGI",
        "https://businessprocedures.bf/objective/1?l=fr",
        "https://dgi.bf/regime_imposition/",
        "https://servicepublic.gov.bf/fiches/impots‚Äëet‚Äëtaxes‚Äëimpot‚Äësur‚Äëles‚Äësocietes‚Äëis",
        "https://servicepublic.gov.bf/fiches/impots‚Äëet‚Äëtaxes‚Äëimpot‚Äësur‚Äëles‚Äëbenefices‚Äënon‚Äëcommerciaux‚Äëibnc",
        "https://servicepublic.gov.bf/fiches/impots‚Äëet‚Äëtaxes‚Äëtaxe‚Äësur‚Äëla‚Äëvaleur-ajoutee-tva",
        "https://servicepublic.gov.bf/fiches/impots‚Äëet‚Äëtaxes‚Äëimpot‚Äësur‚Äëles‚Äërevenus‚Äëfonciers‚Äëirf",
        "https://servicepublic.gov.bf/fiches/impots‚Äëet‚Äëtaxes‚Äëcontribution‚Äëdes‚Äëpatentes",
        "https://investburkina.com/doc/ABI-avantages_fiscaux_code-fran.pdf",
        "https://www.finances.gov.bf/fileadmin/user_upload/storage/fichiers/LIVRET_SUR_LES_MESURES_FISCALES_NOUVELLES_2023.pdf"
    ]
}

   
    
    # Si vous avez des URLs, les scraper
    if any(urls_to_scrape.values()):
        collector.scrape_multiple_urls(urls_to_scrape)
    
    # √âTAPE 2: PDFs √† t√©l√©charger (si vous avez des liens)
    pdfs_to_download = [
        # ("url_pdf", "nom_fichier.pdf"),
    ]
    
    for pdf_url, filename in pdfs_to_download:
        filepath = collector.download_pdf(pdf_url, filename)
        if filepath:
            time.sleep(2)
    
    # Traiter les PDFs t√©l√©charg√©s
    collector.process_pdfs()
    
    # √âTAPE 3: G√©n√©rer des donn√©es synth√©tiques pour atteindre 500+
    # Ajustez le nombre selon ce que vous avez d√©j√† collect√©
    needed = max(0, 500 - len(collector.corpus))
    if needed > 0:
        collector.generate_synthetic_data(needed)
    
    # √âTAPE 4: Sauvegarder
    collector.save_corpus()
    collector.save_sources()
    
    # Afficher statistiques
    collector.print_statistics()
    
    print("\n‚úÖ COLLECTE TERMIN√âE!")
    print("üìÅ Fichiers cr√©√©s:")
    print("   - data/corpus.json")
    print("   - data/sources.txt")
    print("   - data/pdfs/ (si PDFs t√©l√©charg√©s)")


if __name__ == "__main__":
    main()